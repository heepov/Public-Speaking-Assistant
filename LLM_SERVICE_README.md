# LLM Processing Service

–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLaMA 3 13B Q4_K_M GGUF –º–æ–¥–µ–ª–∏ —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π.

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **LLaMA 3 13B Q4_K_M GGUF**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
- **GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ**: –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ CUDA –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **RoPE Scaling**: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 32k —Ç–æ–∫–µ–Ω–æ–≤
- **–ì–∏–±–∫–∏–π –≤–≤–æ–¥**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ JSON, TXT, MD –∏ –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
- **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏**: –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
- **REST API**: –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **GPU**: NVIDIA GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA 12.1+
- **RAM**: –ú–∏–Ω–∏–º—É–º 16GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 32GB+)
- **VRAM**: –ú–∏–Ω–∏–º—É–º 8GB –¥–ª—è –º–æ–¥–µ–ª–∏ 13B
- **Docker**: –í–µ—Ä—Å–∏—è 20.10+
- **NVIDIA Container Toolkit**: –î–ª—è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
LLM Processing Service
‚îú‚îÄ‚îÄ FastAPI (–ø–æ—Ä—Ç 8003)
‚îú‚îÄ‚îÄ LLaMA 3 13B Q4_K_M GGUF
‚îú‚îÄ‚îÄ GPU Acceleration (CUDA)
‚îú‚îÄ‚îÄ RoPE Scaling (32k context)
‚îî‚îÄ‚îÄ File Processing
    ‚îú‚îÄ‚îÄ JSON input
    ‚îú‚îÄ‚îÄ TXT/MD files
    ‚îú‚îÄ‚îÄ Instructions (MD)
    ‚îî‚îÄ‚îÄ Output results
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–∞

```powershell
.\scripts\docker-build-llm.ps1
```

### 2. –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞

```powershell
.\scripts\docker-run-llm.ps1 -Port 8003 -GpuId 0
```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

```bash
curl http://localhost:8003/health
```

## üìñ API Endpoints

### POST /process
–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `input_file`: –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ (JSON, TXT, MD)
- `task_id`: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
- `prompt`: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- `instructions_file`: –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞:**
```bash
curl -X POST "http://localhost:8003/process" \
  -F "input_file=@example_input.json" \
  -F "task_id=task_123" \
  -F "prompt=–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
```

### POST /process-json
–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `data`: JSON –¥–∞–Ω–Ω—ã–µ
- `task_id`: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
- `prompt`: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- `instructions_file`: –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

### GET /health
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞

### GET /model-info
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏

### GET /supported-formats
–°–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
app/features/llm_processing/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ service.py              # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å
‚îú‚îÄ‚îÄ microservice.py         # FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile          # Docker –æ–±—Ä–∞–∑
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îÇ   ‚îî‚îÄ‚îÄ download_model.sh   # –°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ example_input.json      # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚îî‚îÄ‚îÄ example_instructions.md # –ü—Ä–∏–º–µ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
```

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
PORT=8003                    # –ü–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–∞
LOG_LEVEL=INFO              # –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
CUDA_VISIBLE_DEVICES=0      # ID GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
LLAMA_CUBLAS=1              # –í–∫–ª—é—á–∏—Ç—å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
MODEL_PATH=/models/llama-3-13b-q4_k_m.gguf
CONTEXT_LENGTH=32768        # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏

```python
# –í service.py
self.model = Llama(
    model_path=str(self.model_path),
    n_gpu_layers=-1,        # –í—Å–µ —Å–ª–æ–∏ –Ω–∞ GPU
    n_ctx=self.context_length,
    rope_scaling_type=1,    # RoPE scaling
    rope_freq_scale=0.5,    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è 32k
    verbose=False
)
```

## üìä –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –ê–Ω–∞–ª–∏–∑ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è

**–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (example_input.json):**
```json
{
  "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ",
  "speaker": "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
  "content": {
    "introduction": "–î–æ–±—Ä—ã–π –¥–µ–Ω—å, —É–≤–∞–∂–∞–µ–º—ã–µ –∫–æ–ª–ª–µ–≥–∏!...",
    "main_points": [...],
    "conclusion": "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ!"
  }
}
```

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (example_instructions.md):**
```markdown
# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏
- –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (40%)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (25%)
- –ü–æ–¥–∞—á–∞ (20%)
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã (15%)
```

**–ü—Ä–æ–º–ø—Ç:**
```
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
```

### 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

**–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (text.txt):**
```
–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. 
LLM –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –∏ –¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
```

**–ü—Ä–æ–º–ø—Ç:**
```
–ù–∞–π–¥–∏ –æ—à–∏–±–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è
```

## üê≥ Docker Compose

–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ –æ–±—â–∏–π docker-compose.yml:

```yaml
llm-processing:
  build:
    context: ../..
    dockerfile: app/features/llm_processing/docker/Dockerfile
  image: pres-prog-llm:latest
  container_name: pres-prog-llm
  ports:
    - "8003:8003"
  environment:
    - CUDA_VISIBLE_DEVICES=0
    - LLAMA_CUBLAS=1
  volumes:
    - ../../models:/models
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –õ–æ–≥–∏

```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
docker logs pres-prog-llm

# –õ–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
docker logs -f pres-prog-llm
```

### –ú–µ—Ç—Ä–∏–∫–∏

- **–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏**: –û–±—ã—á–Ω–æ 10-30 —Å–µ–∫—É–Ω–¥
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU**: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ `nvidia-smi`
- **–ü–∞–º—è—Ç—å**: ~8GB VRAM –¥–ª—è –º–æ–¥–µ–ª–∏ 13B

## üõ†Ô∏è –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
ls -la models/llama-3-13b-q4_k_m.gguf

# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤—Ä—É—á–Ω—É—é
./app/features/llm_processing/docker/download_model.sh
```

### –ü—Ä–æ–±–ª–µ–º–∞: GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CUDA
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
docker exec pres-prog-llm env | grep CUDA
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏

```bash
# –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —Å–ª–æ–µ–≤
# –í service.py –∏–∑–º–µ–Ω–∏—Ç–µ n_gpu_layers —Å -1 –Ω–∞ –º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
```

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –¢–µ—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–æ–¥–µ–ª—å | VRAM | –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ | –¢–æ—á–Ω–æ—Å—Ç—å |
|--------|------|-----------------|----------|
| LLaMA 3 13B Q4_K_M | 8GB | 15-25 —Å–µ–∫ | –í—ã—Å–æ–∫–∞—è |
| LLaMA 3 13B Q4_K_M | 16GB | 10-15 —Å–µ–∫ | –í—ã—Å–æ–∫–∞—è |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

1. **GPU —Å–ª–æ–∏**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `n_gpu_layers=-1` –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
2. **–ö–æ–Ω—Ç–µ–∫—Å—Ç**: –£–≤–µ–ª–∏—á—å—Ç–µ `n_ctx` –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
3. **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º**: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

## üîê –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

- **–ò–∑–æ–ª—è—Ü–∏—è**: –ö–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –í—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è
- **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∞—É–¥–∏—Ç–∞
- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

## ü§ù –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –° –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º

```python
import requests

def process_with_llm(input_data, prompt, task_id):
    url = "http://localhost:8003/process"
    files = {
        'input_file': ('data.json', json.dumps(input_data)),
        'instructions_file': ('instructions.md', instructions_content)
    }
    data = {
        'task_id': task_id,
        'prompt': prompt
    }
    
    response = requests.post(url, files=files, data=data)
    return response.json()
```

### Webhook –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```python
@app.post("/webhook/llm-result")
async def handle_llm_result(result: dict):
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç LLM —Å–µ—Ä–≤–∏—Å–∞
    task_id = result['task_id']
    processed_result = result['result']
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–ª–∏ –¥–∞–ª—å–Ω–µ–π—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    await save_result(task_id, processed_result)
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [LLaMA 3 Documentation](https://llama.meta.com/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [Docker GPU Support](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)

## üÜò –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `docker logs pres-prog-llm`
2. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ GPU: `nvidia-smi`
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–¥–µ–ª—å: `ls -la models/`
4. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—ã—à–µ

---

**–í–µ—Ä—Å–∏—è**: 1.0.0  
**–î–∞—Ç–∞**: 2024-01-15  
**–ê–≤—Ç–æ—Ä**: Media Processor Team
