"""
–°–µ—Ä–≤–∏—Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º WhisperX
"""

import os
import torch
import whisperx
import librosa
import numpy as np
from pathlib import Path
from typing import Optional, Callable, Dict, List, Any
from datetime import datetime
import tempfile
import gc
import json

from app.core.logger import setup_logger
from app.core.config import settings

logger = setup_logger(__name__)


class TranscriptionService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        self.model = None
        self.align_model = None
        self.diarize_model = None
        self.device = None
        self.compute_type = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        self.model_size = "base"  # tiny, base, small, medium, large-v2, large-v3
        self.language = "ru"      # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        self._setup_model_cache()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–∞—É–∑
        self.pause_settings = {
            'min_pause_duration': 0.2,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑—ã (—Å–µ–∫)
            'energy_threshold': 0.005,  # –ü–æ—Ä–æ–≥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏
            'frame_length': 1024,       # –î–ª–∏–Ω–∞ –∫–∞–¥—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'hop_length': 256          # –®–∞–≥ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏
        }
    
    def _setup_model_cache(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cache_dir = os.getenv('WHISPERX_CACHE_DIR', '/app/app/features/transcription/models_cache')
        hf_home = os.getenv('HF_HOME', '/app/app/features/transcription/models_cache')
        transformers_cache = os.getenv('TRANSFORMERS_CACHE', '/app/app/features/transcription/models_cache')
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        for cache_path in [cache_dir, hf_home, transformers_cache]:
            Path(cache_path).mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üìÅ Model cache directory: {cache_dir}")
        logger.info(f"üìÅ HuggingFace cache: {hf_home}")
        logger.info(f"üìÅ Transformers cache: {transformers_cache}")
    
    def is_audio_file(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∞—É–¥–∏–æ"""
        return file_path.suffix.lower() in settings.SUPPORTED_AUDIO_FORMATS
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
        logger.info("üöÄ Initializing transcription service...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ —Ç–∏–ø –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        self._setup_device()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Whisper
        await self._load_whisper_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        await self._load_alignment_model()
        
        logger.info("‚úÖ Transcription service successfully initialized")
    
    def _setup_device(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if torch.cuda.is_available():
            self.device = "cuda"
            self.compute_type = "float16"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è GPU
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            logger.info(f"üöÄ GPU available: {gpu_name} ({gpu_memory:.1f} GB)")
            logger.info(f"üéØ Using GPU for transcription acceleration")
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
            torch.cuda.empty_cache()
            
        else:
            self.device = "cpu"
            self.compute_type = "int8"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è CPU
            logger.info("‚ÑπÔ∏è CUDA not available, using CPU")
        
        logger.info(f"üñ•Ô∏è Transcription device: {self.device.upper()}")
        logger.info(f"‚öôÔ∏è Compute type: {self.compute_type}")
    
    async def _load_whisper_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ WhisperX"""
        logger.info(f"üì• Loading Whisper model on {self.device.upper()}: {self.model_size}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∫—ç—à—É –º–æ–¥–µ–ª–µ–π
            cache_dir = os.getenv('WHISPERX_CACHE_DIR', '/app/app/features/transcription/models_cache')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞
            self.model = whisperx.load_model(
                self.model_size,
                device=self.device,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                compute_type=self.compute_type,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–∏–ø –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
                language=self.language,
                download_root=cache_dir  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            )
            
            logger.info(f"‚úÖ Whisper model {self.model_size} loaded on {self.device.upper()}")
            logger.info(f"üìÅ Model cached in: {cache_dir}")
            
        except Exception as e:
            logger.error(f"‚ùå Error loading Whisper model: {e}")
            raise RuntimeError(f"Failed to load Whisper model: {e}")
    
    async def _load_alignment_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–ª–æ–≤"""
        logger.info(f"üì• Loading word alignment model on {self.device.upper()}...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∫—ç—à—É –º–æ–¥–µ–ª–µ–π
            cache_dir = os.getenv('WHISPERX_CACHE_DIR', '/app/app/features/transcription/models_cache')
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            self.align_model, metadata = whisperx.load_align_model(
                language_code=self.language,
                device=self.device,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                cache_dir=cache_dir  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            )
            
            logger.info(f"‚úÖ Alignment model loaded on {self.device.upper()}")
            logger.info(f"üìÅ Alignment model cached in: {cache_dir}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load alignment model for Russian: {e}")
            
            try:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                logger.info(f"üîÑ Trying to load universal alignment model on {self.device.upper()}...")
                self.align_model, metadata = whisperx.load_align_model(
                    language_code="en",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ fallback
                    device=self.device,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                    cache_dir=cache_dir  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                )
                logger.info(f"‚úÖ Universal alignment model loaded on {self.device.upper()}")
                
            except Exception as e2:
                logger.warning(f"‚ö†Ô∏è Failed to load universal alignment model: {e2}")
                self.align_model = None
    
    def is_cuda_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA"""
        return torch.cuda.is_available()
    
    def get_device_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ"""
        info = {
            "device": self.device,
            "compute_type": self.compute_type,
            "cuda_available": torch.cuda.is_available()
        }
        
        if torch.cuda.is_available():
            info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_cached": torch.cuda.memory_reserved(0)
            })
        
        return info
    
    def _detect_pauses(
        self,
        audio_path: Path,
        log_callback: Optional[Callable[[str, str], None]] = None
    ) -> List[Dict]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—É–∑ –≤ –∞—É–¥–∏–æ —Ñ–∞–π–ª–µ
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            log_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞—É–∑–∞—Ö
        """
        
        def log(level: str, message: str):
            if log_callback:
                log_callback(level, message)
            logger.debug(message)
        
        log("INFO", "üîç Analyzing pauses in audio...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            y, sr = librosa.load(str(audio_path), sr=None)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —ç–Ω–µ—Ä–≥–∏—é —Å–∏–≥–Ω–∞–ª–∞
            frame_length = self.pause_settings['frame_length']
            hop_length = self.pause_settings['hop_length']
            
            # RMS —ç–Ω–µ—Ä–≥–∏—è
            rms = librosa.feature.rms(
                y=y,
                frame_length=frame_length,
                hop_length=hop_length
            )[0]
            
            # –í—Ä–µ–º–µ–Ω–∞ –∫–∞–¥—Ä–æ–≤
            times = librosa.frames_to_time(
                range(len(rms)),
                sr=sr,
                hop_length=hop_length
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            threshold = self.pause_settings['energy_threshold']
            
            # –ù–∞—Ö–æ–¥–∏–º —É—á–∞—Å—Ç–∫–∏ —Å –Ω–∏–∑–∫–æ–π —ç–Ω–µ—Ä–≥–∏–µ–π (–ø–∞—É–∑—ã)
            is_pause = rms < threshold
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ –ø–∞—É–∑—ã
            pauses = []
            pause_start = None
            
            for i, (time, pause) in enumerate(zip(times, is_pause)):
                if pause and pause_start is None:
                    # –ù–∞—á–∞–ª–æ –ø–∞—É–∑—ã
                    pause_start = time
                elif not pause and pause_start is not None:
                    # –ö–æ–Ω–µ—Ü –ø–∞—É–∑—ã
                    pause_duration = time - pause_start
                    
                    if pause_duration >= self.pause_settings['min_pause_duration']:
                        pauses.append({
                            'start': pause_start,
                            'end': time,
                            'duration': pause_duration
                        })
                    
                    pause_start = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–∞—É–∑—É
            if pause_start is not None:
                pause_duration = times[-1] - pause_start
                if pause_duration >= self.pause_settings['min_pause_duration']:
                    pauses.append({
                        'start': pause_start,
                        'end': times[-1],
                        'duration': pause_duration
                    })
            
            log("INFO", f"üîç Detected pauses: {len(pauses)}")
            
            return pauses
            
        except Exception as e:
            log("WARNING", f"‚ö†Ô∏è Error analyzing pauses: {e}")
            return []
    
    async def transcribe_file(
        self,
        file_path: Path,
        file_extension: str,
        task_id: str,
        progress_callback: Optional[Callable[[int, str], None]] = None,
        log_callback: Optional[Callable[[str, str], None]] = None
    ) -> Dict[str, str]:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—Ü–∏—è –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            file_extension: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            task_id: ID –∑–∞–¥–∞—á–∏
            progress_callback: –§—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            log_callback: –§—É–Ω–∫—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        
        def log(level: str, message: str):
            if log_callback:
                log_callback(level, message)
            logger.info(message)
        
        def update_progress(percent: int, message: str):
            if progress_callback:
                progress_callback(percent, message)
        
        log("INFO", f"üé§ Starting file transcription: {file_path.name}")
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        audio_file_path = None
        temp_audio_path = None
        
        try:
            update_progress(10, "Preparing audio...")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∞—É–¥–∏–æ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if file_extension.lower() in ['.mp4', '.avi', '.mov', '.mkv']:
                log("INFO", "üìπ Video file detected, but video conversion is not supported in this service.")
                log("INFO", "Please use the video_to_audio service first to convert video to audio.")
                raise RuntimeError("Video files are not supported. Please convert video to audio first using the video_to_audio service.")
            else:
                log("INFO", "üéµ Using original audio file")
                audio_file_path = file_path
            
            update_progress(35, "Analyzing pauses...")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–∞—É–∑
            pauses = self._detect_pauses(audio_file_path, log_callback)
            
            update_progress(40, "Loading audio into Whisper...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –¥–ª—è Whisper
            audio = whisperx.load_audio(str(audio_file_path))
            
            update_progress(50, "Performing transcription...")
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            log("INFO", f"üéØ Performing transcription (model: {self.model_size}, device: {self.device})")
            
            result = self.model.transcribe(
                audio,
                batch_size=16 if self.device == "cuda" else 4
            )
            
            update_progress(75, "Aligning words...")
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–ª–æ–≤ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞)
            if self.align_model is not None:
                try:
                    log("INFO", "üéØ Aligning words with timestamps...")
                    
                    result = whisperx.align(
                        result["segments"],
                        self.align_model,
                        whisperx.utils.get_writer_output_format("json"),
                        audio,
                        self.device,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                        return_char_alignments=False
                    )
                    
                    log("INFO", "‚úÖ Word alignment completed successfully")
                    
                except Exception as e:
                    log("WARNING", f"‚ö†Ô∏è Alignment error: {e}")
                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é —Ä–∞–∑–±–∏–≤–∫—É –ø–æ —Å–ª–æ–≤–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                    log("INFO", "üîÑ Creating simple word breakdown...")
                    result = self._create_simple_word_alignment(result)
            else:
                log("INFO", "üîÑ Alignment model not available, creating simple word breakdown...")
                result = self._create_simple_word_alignment(result)
            
            update_progress(90, "Formatting result...")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            formatted_results = self._format_transcription_result(
                result,
                pauses,
                file_path.name,
                task_id
            )
            
            update_progress(100, "Transcription completed!")
            
            log("INFO", "‚úÖ Transcription completed successfully")
            
            return formatted_results
            
        except Exception as e:
            log("ERROR", f"‚ùå Transcription error: {e}")
            raise RuntimeError(f"Transcription error: {e}")
            
        finally:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ —Ñ–∞–π–ª (–Ω–µ —É–¥–∞–ª—è–µ–º)
            if temp_audio_path and temp_audio_path.exists():
                log("INFO", f"üíæ Temporary audio file saved: {temp_audio_path}")
            
            # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
    
    async def transcribe_audio_file(
        self,
        audio_path: Path,
        task_id: str
    ) -> Dict[str, Any]:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—Ü–∏—è –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            task_id: ID –∑–∞–¥–∞—á–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
        """
        
        logger.info(f"üé§ Starting audio transcription: {audio_path.name}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            file_extension = audio_path.suffix.lower()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            formatted_results = await self.transcribe_file(
                file_path=audio_path,
                file_extension=file_extension,
                task_id=task_id
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
            txt_filename = f"{task_id}_transcription.txt"
            json_filename = f"{task_id}_transcription.json"
            
            txt_path = settings.OUTPUT_DIR / txt_filename
            json_path = settings.OUTPUT_DIR / json_filename
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(formatted_results['simple_text'])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º timeline —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON
            json_data = {
                "timeline": formatted_results['timeline']
            }
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ Transcription files saved: {txt_filename}, {json_filename}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö
            return {
                "status": "success",
                "task_id": task_id,
                "transcription": formatted_results['simple_text'],
                "timeline": formatted_results['timeline'],
                "saved_files": {
                    "txt": txt_filename,
                    "json": json_filename
                },
                "model_used": self.model_size,
                "device_used": self.device,
                "language": self.language
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in transcribe_audio_file: {e}")
            raise RuntimeError(f"Transcription failed: {str(e)}")
    
    def _create_simple_word_alignment(self, result: Dict) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ —Å–ª–æ–≤–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –æ—Ç Whisper
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –ø–æ —Å–ª–æ–≤–∞–º
        """
        
        segments = result.get("segments", [])
        
        for segment in segments:
            text = segment.get('text', '').strip()
            start_time = segment.get('start', 0)
            end_time = segment.get('end', 0)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            import re
            words = re.findall(r'\b\w+\b|[^\w\s]', text)
            if not words:
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
            total_duration = end_time - start_time
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –ø—Ä–æ–∏–∑–Ω–æ—Å—è—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ
            word_durations = []
            for word in words:
                if re.match(r'[^\w\s]', word):  # –ó–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                    word_durations.append(0.1)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                else:
                    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ (0.3-0.8 —Å–µ–∫—É–Ω–¥—ã)
                    word_durations.append(0.5)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –æ–Ω–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ –æ–±—â–µ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞
            total_word_duration = sum(word_durations)
            if total_word_duration > 0:
                scale_factor = total_duration / total_word_duration
                word_durations = [d * scale_factor for d in word_durations]
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
            word_list = []
            current_time = start_time
            
            for i, word in enumerate(words):
                word_end_time = current_time + word_durations[i]
                
                word_list.append({
                    'word': word,
                    'start': current_time,
                    'end': word_end_time
                })
                
                current_time = word_end_time
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–±–∏–≤–∫—É –ø–æ —Å–ª–æ–≤–∞–º –≤ —Å–µ–≥–º–µ–Ω—Ç
            segment['words'] = word_list
        
        return result
    
    def _format_transcription_result(
        self,
        whisper_result: Dict,
        pauses: List[Dict],
        filename: str,
        task_id: str
    ) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞
        
        Args:
            whisper_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç WhisperX
            pauses: –°–ø–∏—Å–æ–∫ –ø–∞—É–∑
            filename: –ò–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            task_id: ID –∑–∞–¥–∞—á–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ timeline –¥–∞–Ω–Ω—ã–º–∏
        """
        
        segments = whisper_result.get("segments", [])
        
        # 1. –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç (—Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
        full_text = " ".join(
            segment.get('text', '').strip()
            for segment in segments
        ).strip()
        
        # 2. Timeline –¥–∞–Ω–Ω—ã–µ –¥–ª—è JSON
        timeline = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        for segment in segments:
            words = segment.get('words', [])
            
            for word_info in words:
                timeline.append({
                    "type": "word",
                    "text": word_info.get('word', '').strip(),
                    "start": word_info.get('start', 0),
                    "end": word_info.get('end', 0)
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—ã
        for pause in pauses:
            timeline.append({
                "type": "pause",
                "start": pause['start'],
                "end": pause['end'],
                "duration": pause['duration']
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º timeline –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
        timeline.sort(key=lambda x: x['start'])
        
        return {
            'simple_text': full_text,
            'timeline': timeline
        }
    
    def _format_time(self, seconds: float) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤ MM:SS.ss —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            seconds: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
        """
        
        minutes = int(seconds // 60)
        secs = seconds % 60
        
        return f"{minutes:02d}:{secs:05.2f}"
