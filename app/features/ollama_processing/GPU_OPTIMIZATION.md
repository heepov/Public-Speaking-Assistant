# üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –¥–ª—è Ollama

## üìã –ß—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ

–í –∫–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ GPU –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ GPU –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU.

### üîß –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

- **`num_gpu_layers`** - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (–æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ GPU)
- **`num_ctx`** - —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–≤–ª–∏—è–µ—Ç –Ω–∞ –ø–∞–º—è—Ç—å)
- **`num_thread`** - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU –ø–æ—Ç–æ–∫–æ–≤

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è RTX 2080 Ti (11 GB)

### ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

```json
{
  "num_gpu_layers": 32,
  "num_ctx": 4096,
  "num_thread": 8,
  "temperature": 0.95,
  "top_p": 0.9,
  "num_predict": 800
}
```

### üîç –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É GPU

1. **Windows (PowerShell):**
```powershell
nvidia-smi -l 1
```

2. **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤ –∫–æ–¥–µ:**
```python
import torch
print(f"GPU –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"GPU –ø–∞–º—è—Ç—å (–≤—Å–µ–≥–æ): {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç:

```bash
cd app/features/ollama_processing
python test_gpu_optimization.py
```

## üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

–ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞—à—É GPU –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

| GPU –ü–∞–º—è—Ç—å | num_gpu_layers | num_ctx | num_thread |
|------------|----------------|---------|------------|
| ‚â•24 GB     | 40            | 8192    | 12         |
| ‚â•16 GB     | 35            | 6144    | 10         |
| ‚â•12 GB     | 32            | 4096    | 8          |
| ‚â•8 GB      | 28            | 3072    | 6          |
| <8 GB      | 20            | 2048    | 4          |

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã

1. **–î–ª—è gpt-oss:20b** - –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `num_gpu_layers: 32` –º–∞–∫—Å–∏–º—É–º
2. **–ï—Å–ª–∏ –≤—ã–ª–µ—Ç–∞–µ—Ç –ø–æ –ø–∞–º—è—Ç–∏** - —É–º–µ–Ω—å—à–∏—Ç–µ `num_gpu_layers` –Ω–∞ 4-8
3. **–î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏** - –Ω–∞—á–Ω–∏—Ç–µ —Å `num_gpu_layers: 28`

## üîß –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:

```json
{
  "num_gpu_layers": 32,
  "num_ctx": 4096,
  "num_thread": 8,
  "temperature": 0.95,
  "top_p": 0.9,
  "num_predict": 800
}
```

–ò –ø–µ—Ä–µ–¥–∞–π—Ç–µ –µ–≥–æ –≤ `model_params` –ø—Ä–∏ –≤—ã–∑–æ–≤–µ `process_text()`.

## üö® –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞: "CUDA out of memory"
**–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç–µ `num_gpu_layers` –Ω–∞ 4-8

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–∏–∑–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GPU
**–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á—å—Ç–µ `num_gpu_layers` (–Ω–æ –Ω–µ –±–æ–ª—å—à–µ 40)

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞
**–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á—å—Ç–µ `num_thread` –∏ `num_ctx`

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–î–æ–±–∞–≤—å—Ç–µ –≤ –∫–æ–¥ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

```python
import torch
import psutil

# GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
gpu_memory = torch.cuda.memory_allocated() / 1024**3
print(f"GPU –ø–∞–º—è—Ç—å: {gpu_memory:.2f} GB")

# CPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
cpu_percent = psutil.cpu_percent()
print(f"CPU –∑–∞–≥—Ä—É–∑–∫–∞: {cpu_percent}%")
```
