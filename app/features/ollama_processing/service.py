"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
"""

import os
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import asyncio
from datetime import datetime
import requests

import torch
import ollama

from app.core.config import settings
from app.core.logger import setup_logger

logger = setup_logger(__name__)


class OllamaProcessingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama"""
    
    def __init__(self):
        self.ollama_client = None
        self.is_initialized = False
        self.available_models = []
        self.default_model = "llama2"
        self.ollama_host = os.environ.get("OLLAMA_HOST", "localhost:11434")
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama —Å–µ—Ä–≤–∏—Å–∞...")
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            logger.info("=" * 60)
            logger.info("üîç –ü–†–û–í–ï–†–ö–ê GPU –ü–û–î–î–ï–†–ñ–ö–ò")
            logger.info("=" * 60)
            
            # PyTorch CUDA
            logger.info(f"üîç PyTorch CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}")
            logger.info(f"üîç PyTorch CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
            
            if torch.cuda.is_available():
                logger.info(f"üîç PyTorch CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
                logger.info(f"üîç PyTorch CUDA –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                logger.info(f"üîç PyTorch CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
            
            # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            logger.info(f"üîç OLLAMA_HOST: {self.ollama_host}")
            logger.info(f"üîç CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', '–ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–ê')}")
            
            logger.info("=" * 60)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama...")
            try:
                response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
                if response.status_code == 200:
                    logger.info("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
                    models_data = response.json()
                    self.available_models = [model['name'] for model in models_data.get('models', [])]
                    logger.info(f"üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {self.available_models}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞
            logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
            self.ollama_client = ollama.Client(host=f"http://{self.ollama_host}")
            
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
            if not self.available_models:
                logger.info("üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
                try:
                    await self._install_default_model()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {e}")
            
            self.is_initialized = True
            logger.info("‚úÖ Ollama —Å–µ—Ä–≤–∏—Å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ollama —Å–µ—Ä–≤–∏—Å–∞: {e}")
            raise
    
    async def _install_default_model(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ {self.default_model}...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏
            import subprocess
            import asyncio
            
            process = await asyncio.create_subprocess_exec(
                "ollama", "pull", self.default_model,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.default_model} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                self.available_models.append(self.default_model)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {stderr.decode()}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    


    async def process_text_full(
        self,
        prompt: str,
        input_data: Optional[Dict[str, Any]] = None,
        instructions_file: Optional[str] = None,
        task_id: str = "",
        model_name: str = None,
        system_prompt: Optional[str] = None,
        model_params: Optional[Dict[str, Any]] = None,
        instructions_content: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
            input_data: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç, JSON –∏ —Ç.–¥.) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
            instructions_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            task_id: ID –∑–∞–¥–∞—á–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - —Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (temperature, top_p, num_predict, repeat_penalty, presence_penalty –∏ –¥—Ä.)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        
        if not self.is_initialized:
            raise RuntimeError("–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ - task_id: {task_id}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            instructions = ""
            if instructions_content:
                instructions = instructions_content
                logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({len(instructions)} —Å–∏–º–≤–æ–ª–æ–≤)")
            elif instructions_file and Path(instructions_file).exists():
                with open(instructions_file, 'r', encoding='utf-8') as f:
                    instructions = f.read()
                logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞: {instructions_file} ({len(instructions)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            input_text = ""
            if input_data is not None:
                input_text = self._prepare_input(input_data)
                logger.info(f"üìÑ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç ({len(input_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
            if system_prompt is None:
                system_prompt = self._create_default_system_prompt(instructions)
            elif instructions and instructions.strip():
                system_prompt = f"{system_prompt}\n\n–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{instructions}"
            
            user_prompt = self._create_user_prompt(input_text, prompt)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama
            model_to_use = model_name or self.default_model
            logger.info(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {model_to_use}")
            result = await self._process_with_ollama(system_prompt, user_prompt, model_to_use, task_id, model_params)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_path = self._save_result(result, task_id)
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            
            return {
                "task_id": task_id,
                "status": "completed",
                "result": result,
                "output_file": str(output_path),
                "timestamp": datetime.now().isoformat(),
                "model": model_to_use,
                "service": "ollama"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise
    
    async def _process_with_ollama(self, system_prompt: str, user_prompt: str, model_name: str, task_id: str, model_params: Optional[Dict[str, Any]] = None) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é Ollama"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å Ollama...")
            start_time = datetime.now()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_name}")
            try:
                models_response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
                if models_response.status_code == 200:
                    available_models = [model['name'] for model in models_response.json().get('models', [])]
                    if model_name not in available_models:
                        logger.warning(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available_models}")
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                        if available_models:
                            model_name = available_models[0]
                            logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å: {model_name}")
                        else:
                            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
            logger.info("=" * 80)
            logger.info("üîç –ü–û–õ–ù–´–ô –ü–†–û–ú–ü–¢ –î–õ–Ø OLLAMA:")
            logger.info(f"üìù –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç ({len(full_prompt)} —Å–∏–º–≤–æ–ª–æ–≤):")
            logger.info("-" * 40)
            logger.info(full_prompt)
            logger.info("=" * 80)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            try:
                calculated_ctx = int((len(full_prompt) + (len(full_prompt) * 0.5)) / 4)
                if calculated_ctx < 8192:
                    num_ctx = 8192
                else:
                    num_ctx = calculated_ctx
                logger.info(f"üîß –í—ã—á–∏—Å–ª–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {calculated_ctx}, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {num_ctx}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8192")
                num_ctx = 8192
            
            default_options = {
                "num_predict": -1,
                "num_ctx": num_ctx,       # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                "num_gpu_layers": 35,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU
                "num_thread": 12        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU –ø–æ—Ç–æ–∫–æ–≤
            }
            

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            logger.info(f"üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama:")
            logger.info(f"üîç model_name: {model_name}")
            logger.info(f"üîç system_prompt (—Ä–∞–∑–º–µ—Ä: {len(system_prompt)}): '{system_prompt[:200]}...'")
            logger.info(f"üîç user_prompt (—Ä–∞–∑–º–µ—Ä: {len(user_prompt)}): '{user_prompt[:200]}...'")
            
            response = self.ollama_client.chat(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                options=default_options
            )
            logger.info(f"üîç –ó–∞–ø—Ä–æ—Å –∫ Ollama: model_name={model_name},\n system_prompt={system_prompt},\n user_prompt={user_prompt}\n options={default_options}")
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            result = response['message']['content'].strip()
            
            logger.info(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {inference_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å Ollama: {e}")
            logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {type(e).__name__}")
            raise
    
    def _prepare_input(self, input_data: Dict[str, Any]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if isinstance(input_data, dict):
            return json.dumps(input_data, ensure_ascii=False, indent=2)
        elif isinstance(input_data, str):
            return input_data
        else:
            return str(input_data)
    
    def _create_default_system_prompt(self, instructions: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        logger.info(f"üîç –í–•–û–î –í _create_default_system_prompt —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏: '{instructions[:50] if instructions else 'None'}...'")
        
        base_prompt = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."""
        
        logger.info(f"üìñ –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: '{instructions[:50] if instructions else 'None'}...' (—Ç–∏–ø: {type(instructions)}, –¥–ª–∏–Ω–∞: {len(instructions) if instructions else 0})")
        
        if instructions and instructions.strip():
            final_prompt = f"{base_prompt}\n\n–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{instructions}"
            logger.info(f"üìñ –°–æ–∑–¥–∞–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (—Ä–∞–∑–º–µ—Ä: {len(final_prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
            logger.info(f"üìñ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: '{final_prompt[:200]}...'")
            return final_prompt
        
        logger.info("üìñ –°–æ–∑–¥–∞–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π")
        return base_prompt
    
    def _create_user_prompt(self, input_text: str, prompt: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        if input_text:
            return f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n{input_text}\n\n–ó–∞–¥–∞—á–∞: {prompt}"
        else:
            return f"–ó–∞–¥–∞—á–∞: {prompt}"
    
    def _save_result(self, result: str, task_id: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
        output_dir = Path(settings.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"ollama_result_{task_id}_{timestamp}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result)
        
        return output_file
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            "service": "ollama",
            "ollama_host": self.ollama_host,
            "available_models": self.available_models,
            "default_model": self.default_model,
            "is_initialized": self.is_initialized,
            "gpu_available": torch.cuda.is_available()
        }
    
    async def list_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                return [model['name'] for model in models_data.get('models', [])]
            else:
                return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    
    async def install_model(self, model_name: str) -> bool:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
            
            import subprocess
            import asyncio
            
            process = await asyncio.create_subprocess_exec(
                "ollama", "pull", model_name,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                self.available_models = await self.list_models()
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {stderr.decode()}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
