"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
"""

import os
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import asyncio
from datetime import datetime
import requests

import torch
import ollama
from openai import OpenAI

from app.core.config import settings
from app.core.logger import setup_logger

logger = setup_logger(__name__)


class OllamaProcessingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama"""
    
    def __init__(self):
        self.ollama_client = None
        self.openai_client = None
        self.is_initialized = False
        self.available_models = []
        self.default_model = "llama2"
        self.ollama_host = os.environ.get("OLLAMA_HOST", "localhost:11434")
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama —Å–µ—Ä–≤–∏—Å–∞...")
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            logger.info("=" * 60)
            logger.info("üîç –ü–†–û–í–ï–†–ö–ê GPU –ü–û–î–î–ï–†–ñ–ö–ò")
            logger.info("=" * 60)
            
            # PyTorch CUDA
            logger.info(f"üîç PyTorch CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}")
            logger.info(f"üîç PyTorch CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
            
            if torch.cuda.is_available():
                logger.info(f"üîç PyTorch CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
                logger.info(f"üîç PyTorch CUDA –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                logger.info(f"üîç PyTorch CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
            
            # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            logger.info(f"üîç OLLAMA_HOST: {self.ollama_host}")
            logger.info(f"üîç CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', '–ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–ê')}")
            
            logger.info("=" * 60)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama...")
            try:
                response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
                if response.status_code == 200:
                    logger.info("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
                    models_data = response.json()
                    self.available_models = [model['name'] for model in models_data.get('models', [])]
                    logger.info(f"üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {self.available_models}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞
            logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
            self.ollama_client = ollama.Client(host=f"http://{self.ollama_host}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ OpenAI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            openai_api_key = os.environ.get("OPENAI_API_KEY")
            if openai_api_key:
                logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞...")
                self.openai_client = OpenAI(api_key=openai_api_key)
                logger.info("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
            if not self.available_models:
                logger.info("üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
                try:
                    await self._install_default_model()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {e}")
            
            self.is_initialized = True
            logger.info("‚úÖ Ollama —Å–µ—Ä–≤–∏—Å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ollama —Å–µ—Ä–≤–∏—Å–∞: {e}")
            raise
    
    async def _install_default_model(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ {self.default_model}...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏
            import subprocess
            import asyncio
            
            process = await asyncio.create_subprocess_exec(
                "ollama", "pull", self.default_model,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.default_model} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                self.available_models.append(self.default_model)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {stderr.decode()}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    async def process_text(
        self,
        prompt: str,
        input_data: Optional[Dict[str, Any]] = None,
        instructions_file: Optional[str] = None,
        task_id: str = "",
        model_name: str = None,
        use_openai: bool = False,
        system_prompt: Optional[str] = None,
        model_params: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama –∏–ª–∏ OpenAI
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
            input_data: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç, JSON –∏ —Ç.–¥.) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
            instructions_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            task_id: ID –∑–∞–¥–∞—á–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            use_openai: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI –≤–º–µ—Å—Ç–æ Ollama
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - —Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (temperature, top_p, num_predict, repeat_penalty, presence_penalty –∏ –¥—Ä.)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        
        if not self.is_initialized:
            raise RuntimeError("–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            logger.info("=" * 60)
            logger.info(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–ê - task_id: {task_id}")
            logger.info("=" * 60)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª
            instructions = ""
            if instructions_file and Path(instructions_file).exists():
                with open(instructions_file, 'r', encoding='utf-8') as f:
                    instructions = f.read()
                logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞: {instructions_file}")
                logger.info(f"üìñ –†–∞–∑–º–µ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: {len(instructions)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                logger.info("üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            input_text = ""
            if input_data is not None:
                input_text = self._prepare_input(input_data)
                logger.info(f"üìÑ –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {len(input_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                logger.info("üìÑ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
            if system_prompt is None:
                system_prompt = self._create_default_system_prompt(instructions)
            user_prompt = self._create_user_prompt(input_text, prompt)
            
            logger.info(f"üí¨ –†–∞–∑–º–µ—Ä —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üí¨ –†–∞–∑–º–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {len(user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üí¨ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(system_prompt) + len(user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            if use_openai and self.openai_client:
                logger.info("ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ OpenAI –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                result = await self._process_with_openai(system_prompt, user_prompt, task_id, model_params)
                model_used = "openai-gpt-3.5-turbo"
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama
                model_to_use = model_name or self.default_model
                logger.info(f"ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Ollama –º–æ–¥–µ–ª–∏: {model_to_use}")
                result = await self._process_with_ollama(system_prompt, user_prompt, model_to_use, task_id, model_params)
                model_used = model_to_use
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_path = self._save_result(result, task_id)
            
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            logger.info("=" * 60)
            
            return {
                "task_id": task_id,
                "status": "completed",
                "result": result,
                "output_file": str(output_path),
                "timestamp": datetime.now().isoformat(),
                "model": model_used,
                "service": "openai" if use_openai else "ollama"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise
    
    async def _process_with_ollama(self, system_prompt: str, user_prompt: str, model_name: str, task_id: str, model_params: Optional[Dict[str, Any]] = None) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é Ollama"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å Ollama...")
            start_time = datetime.now()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_name}")
            try:
                models_response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
                if models_response.status_code == 200:
                    available_models = [model['name'] for model in models_response.json().get('models', [])]
                    if model_name not in available_models:
                        logger.warning(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available_models}")
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                        if available_models:
                            model_name = available_models[0]
                            logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å: {model_name}")
                        else:
                            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            default_options = {
                "temperature": 0.95,
                "top_p": 0.9,
                "num_predict": 800,
                "repeat_penalty": 1.2,
                "presence_penalty": 0.8
            }
            
            if model_params:
                default_options.update(model_params)
                logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {model_params}")
            else:
                logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_options}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            response = self.ollama_client.chat(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                options=default_options
            )
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            result = response['message']['content'].strip()
            
            logger.info(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {inference_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å Ollama: {e}")
            logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {type(e).__name__}")
            raise
    
    async def _process_with_openai(self, system_prompt: str, user_prompt: str, task_id: str, model_params: Optional[Dict[str, Any]] = None) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é OpenAI"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å OpenAI...")
            start_time = datetime.now()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            default_params = {
                "max_tokens": 4096,
                "temperature": 0.7,
                "top_p": 0.9
            }
            
            if model_params:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OpenAI
                openai_supported_params = {
                    "max_tokens", "temperature", "top_p", "frequency_penalty", 
                    "presence_penalty", "stop", "n", "stream"
                }
                filtered_params = {k: v for k, v in model_params.items() if k in openai_supported_params}
                default_params.update(filtered_params)
                logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã OpenAI: {filtered_params}")
            else:
                logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã OpenAI –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_params}")
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                **default_params
            )
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            result = response.choices[0].message.content.strip()
            
            logger.info(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {inference_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {response.usage.total_tokens}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å OpenAI: {e}")
            raise
    
    def _prepare_input(self, input_data: Dict[str, Any]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if isinstance(input_data, dict):
            return json.dumps(input_data, ensure_ascii=False, indent=2)
        elif isinstance(input_data, str):
            return input_data
        else:
            return str(input_data)
    
    def _create_default_system_prompt(self, instructions: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        base_prompt = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."""
        
        if instructions:
            return f"{base_prompt}\n\n–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{instructions}"
        
        return base_prompt
    
    def _create_user_prompt(self, input_text: str, prompt: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        if input_text:
            return f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n{input_text}\n\n–ó–∞–¥–∞—á–∞: {prompt}"
        else:
            return f"–ó–∞–¥–∞—á–∞: {prompt}"
    
    def _save_result(self, result: str, task_id: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
        output_dir = Path(settings.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"ollama_result_{task_id}_{timestamp}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result)
        
        return output_file
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            "service": "ollama",
            "ollama_host": self.ollama_host,
            "available_models": self.available_models,
            "default_model": self.default_model,
            "is_initialized": self.is_initialized,
            "openai_available": self.openai_client is not None,
            "gpu_available": torch.cuda.is_available()
        }
    
    async def list_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(f"http://{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                return [model['name'] for model in models_data.get('models', [])]
            else:
                return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    async def install_model(self, model_name: str) -> bool:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
            
            import subprocess
            import asyncio
            
            process = await asyncio.create_subprocess_exec(
                "ollama", "pull", model_name,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                self.available_models = await self.list_models()
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {stderr.decode()}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
