# Ollama Processing Service

## Описание

Микросервис для обработки текста с использованием Ollama и GPU ускорения.

## Ответственность

- **Обработка текста с помощью LLM** - сервис принимает текстовые данные и обрабатывает их через Ollama
- **GPU ускорение** - автоматическое использование CUDA для ускорения обработки
- **Кэширование моделей** - сохранение моделей Ollama между перезапусками
- **Поддержка различных моделей** - любые модели, доступные в Ollama

## Зависимости

- **Входные данные**: Текстовые данные
- **Выходные данные**: Обработанный текст с помощью LLM
- **Внешние зависимости**: 
  - Ollama
  - PyTorch с CUDA
  - FastAPI

## API Endpoints

### POST /process
Обработка текста с помощью LLM

**Параметры:**
- `text`: Входной текст
- `model`: Модель для использования
- `task_id`: ID задачи
- `parameters`: Дополнительные параметры модели

### POST /process/batch
Пакетная обработка нескольких текстовых данных

**Параметры:**
- `texts`: Список текстовых данных
- `model`: Модель для использования
- `task_id`: ID задачи
- `parameters`: Дополнительные параметры модели

### GET /health
Проверка состояния сервиса

### GET /models
Информация о доступных моделях Ollama

### POST /models/pull
Загрузка новой модели

**Параметры:**
- `model_name`: Название модели для загрузки

## Конфигурация

### Переменные окружения
- `OLLAMA_HOST`: Хост Ollama сервера (localhost:11434)
- `CUDA_VISIBLE_DEVICES`: GPU устройства для использования
- `OLLAMA_MODELS`: Директория для кэширования моделей
- `OLLAMA_ORIGINS`: Разрешенные источники для CORS

### Поддерживаемые модели
- Любые модели, доступные в Ollama Hub
- Локальные модели Ollama

## Запуск

### Docker
```bash
# Сборка образа
./docker/scripts/build.ps1

# Запуск контейнера
./docker/scripts/start.ps1

# Проверка состояния
./docker/scripts/check.ps1

# Остановка контейнера
./docker/scripts/stop.ps1
```

### Локально
```bash
cd app/features/ollama_processing
python -m microservice
```

## Архитектура

```
OllamaProcessingService
├── Ollama Client
├── LLM Model (GPU/CPU)
├── Text Processing
└── Result Generation
```

## Мониторинг

- Health check endpoint для проверки состояния
- Логирование всех операций
- Информация о GPU/CPU использовании
- Статистика моделей Ollama

## Безопасность

- Проверка входных данных
- Валидация параметров
- Ограничение размера запросов
- Изоляция процессов

## Web-интерфейс для тестирования

Сервис предоставляет веб-интерфейс для тестирования API:
- URL: http://localhost:8004
- Swagger UI: http://localhost:8004/docs
- ReDoc: http://localhost:8004/redoc

## Кэширование моделей

Модели Ollama сохраняются в volume `/app/app/features/ollama_processing/models_cache` и не перезагружаются при перезапуске контейнера.

## Требования

- Docker с поддержкой GPU (nvidia-docker)
- CUDA 12.1+
- Минимум 8GB RAM
- Достаточно места на диске для моделей
