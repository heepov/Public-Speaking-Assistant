#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ Ollama
"""

import asyncio
import json
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from service import OllamaProcessingService

async def test_gpu_optimization():
    """–¢–µ—Å—Ç GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
    
    service = OllamaProcessingService()
    await service.initialize()
    
    # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU
    print("\n" + "="*60)
    print("üîç –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û GPU")
    print("="*60)
    
    import torch
    if torch.cuda.is_available():
        print(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.get_device_name(0)}")
        print(f"üìä –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"üîß CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
    else:
        print("‚ùå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("\n" + "="*60)
    print("üîß –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–ê–†–ê–ú–ï–¢–†–û–í")
    print("="*60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    test_params = [
        None,  # –ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        {"temperature": 0.7},  # –¢–æ–ª—å–∫–æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
        {"num_gpu_layers": 50},  # –¢–æ–ª—å–∫–æ GPU —Å–ª–æ–∏
        {"num_gpu_layers": 30, "num_ctx": 2048, "temperature": 0.8}  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    ]
    
    for i, params in enumerate(test_params, 1):
        print(f"\nüìã –¢–µ—Å—Ç {i}: {params}")
        optimized = service._optimize_gpu_params(params)
        print(f"üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {optimized}")
    
    # –¢–µ—Å—Ç 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    print("\n" + "="*60)
    print("üì¶ –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    models = await service.list_models()
    if models:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
        for model in models:
            print(f"  - {model}")
    else:
        print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç 4: –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏)
    if models:
        print("\n" + "="*60)
        print("üß™ –¢–ï–°–¢–û–í–´–ô –ó–ê–ü–†–û–°")
        print("="*60)
        
        try:
            result = await service.process_text(
                prompt="–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
                model_name=models[0],
                model_params={"num_gpu_layers": 32, "num_predict": 200}
            )
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω: {len(result['result'])} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"üìÑ –ù–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞: {result['result'][:100]}...")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ: {e}")
    
    print("\n" + "="*60)
    print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*60)

if __name__ == "__main__":
    asyncio.run(test_gpu_optimization())
