"""
–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import torch

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import aiofiles

from app.core.config import settings
from app.core.logger import setup_logger
from app.features.ollama_processing.service import OllamaProcessingService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_logger(__name__)

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
class ModelPullRequest(BaseModel):
    model_name: str

class ProcessRequest(BaseModel):
    text: str
    model: str = "llama2"
    task_id: str
    parameters: Optional[Dict[str, Any]] = None
    system_prompt: Optional[str] = None
    input_file_content: Optional[str] = None
    instructions_file_content: Optional[str] = None

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Ollama Processing Microservice",
    description="–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama",
    version="1.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –ª—É—á—à–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
settings.create_directories()

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
try:
    app.mount("/static", StaticFiles(directory="app/features/ollama_processing/web"), name="static")
except:
    pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ Ollama
ollama_service = None


async def ensure_model_available(model_name: str):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        available_models = await ollama_service.list_models()
        
        if model_name not in available_models:
            logger.info(f"üì• –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º...")
            success = await ollama_service.install_model(model_name)
            if not success:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}")
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        else:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ/—É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global ollama_service
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ Ollama
        ollama_service = OllamaProcessingService()
        await ollama_service.initialize()
        
        logger.info("‚úÖ –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ Ollama: {e}")
        raise


@app.post("/process")
async def process_text(
    request: ProcessRequest
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama (JSON endpoint)
    """
    try:
        logger.info(f"üîÑ –ü–æ–ª—É—á–µ–Ω JSON –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É - task_id: {request.task_id}")
        logger.info(f"ü§ñ –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model}")
        logger.info(f"üìù –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: '{request.text}'")
        logger.info(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(request.text) if request.text else 0}")
        
        start_time = datetime.now()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        await ensure_model_available(request.model)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        parameters = request.parameters or {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (num_predict -> max_tokens)
        if 'num_predict' in parameters and 'max_tokens' not in parameters:
            parameters['max_tokens'] = parameters.pop('num_predict')
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        input_data = None
        if request.input_file_content:
            try:
                input_data = json.loads(request.input_file_content)
            except json.JSONDecodeError:
                input_data = request.input_file_content
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        instructions = request.instructions_file_content
        logger.info(f"üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {instructions[:100] if instructions else '–ù–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π'}...")
        logger.info(f"üìñ –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: {type(instructions)}")
        logger.info(f"üìñ –†–∞–∑–º–µ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: {len(instructions) if instructions else 0} —Å–∏–º–≤–æ–ª–æ–≤")
        if instructions:
            logger.info(f"üìñ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: '{instructions[:200]}...'")
            logger.info(f"üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Å–∏—Å—Ç–µ–º–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É: '{request.system_prompt}'")
        else:
            logger.info(f"üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        logger.info(f"üìñ system_prompt –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: '{request.system_prompt}'")
        result = await ollama_service.process_text_full(
            prompt=request.text,
            input_data=input_data,
            instructions_file=None,
            task_id=request.task_id,
            model_name=request.model,
            use_openai=False,
            system_prompt=request.system_prompt,
            model_params=parameters,
            instructions_content=request.instructions_file_content
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–µ—Å–ª–∏ –Ω–µ –±—ã–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ process_text_full)
        if isinstance(result, dict) and "output_file" in result:
            output_file = Path(result["output_file"])
        else:
            output_file = ollama_service._save_result(result, request.task_id)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        return {
            "status": "success",
            "task_id": request.task_id,
            "result": {
                "processed_text": result.get("result", result),
                "saved_files": {
                    "txt": result.get("output_file", str(output_file))
                }
            },
            "model_used": request.model,
            "device_used": "cuda" if torch.cuda.is_available() else "cpu",
            "processing_time": processing_time
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return {
            "status": "error",
            "error": str(e)
        }

@app.post("/process/form")
async def process_text_form(
    prompt: str = Form(...),
    input_file: Optional[UploadFile] = File(None),
    task_id: str = Form(...),
    instructions_file: Optional[UploadFile] = File(None),
    model_name: str = Form("llama2"),
    use_openai: bool = Form(False),
    system_prompt: Optional[str] = Form(None),
    model_params: Optional[str] = Form(None)
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama –∏–ª–∏ OpenAI
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
    """
    try:
        logger.info(f"üîÑ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É - task_id: {task_id}")
        logger.info(f"ü§ñ –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not use_openai:
            await ensure_model_available(model_name)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        input_data = None
        if input_file:
            content = await input_file.read()
            try:
                input_data = json.loads(content.decode('utf-8'))
            except json.JSONDecodeError:
                input_data = content.decode('utf-8')
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        instructions_content = None
        instructions_path = None
        if instructions_file:
            content = await instructions_file.read()
            instructions_content = content.decode('utf-8')
            logger.info(f"üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞: {instructions_content[:100] if instructions_content else '–ù–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π'}...")
        else:
            logger.info("üìñ –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
        
        # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        parsed_model_params = None
        if model_params:
            try:
                parsed_model_params = json.loads(model_params)
                logger.info(f"üîß –ü–æ–ª—É—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {parsed_model_params}")
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {e}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        result = await ollama_service.process_text_full(
            prompt=prompt,
            input_data=input_data,
            instructions_file=instructions_path,
            task_id=task_id,
            model_name=model_name,
            use_openai=use_openai,
            system_prompt=system_prompt,
            model_params=parsed_model_params,
            instructions_content=instructions_content
        )
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        return {
            "error": str(e),
            "task_id": task_id,
            "status": "error"
        }



@app.post("/process-json")
async def process_json_data(
    prompt: str = Form(...),
    data: Optional[Dict[str, Any]] = None,
    task_id: str = Form(...),
    instructions_file: Optional[UploadFile] = File(None),
    model_name: str = Form("llama2"),
    use_openai: bool = Form(False),
    system_prompt: Optional[str] = Form(None),
    model_params: Optional[str] = Form(None)
):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é
    
    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
        data: JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        task_id: ID –∑–∞–¥–∞—á–∏
        instructions_file: –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        use_openai: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI –≤–º–µ—Å—Ç–æ Ollama
        
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    logger.info(f"üì§ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É JSON (task_id: {task_id})")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
        instructions_path = None
        if instructions_file:
            instructions_filename = f"instructions_{task_id}_{instructions_file.filename}"
            instructions_path = settings.UPLOAD_DIR / instructions_filename
            
            async with aiofiles.open(instructions_path, 'wb') as f:
                content = await instructions_file.read()
                await f.write(content)
            
            logger.info(f"üìñ –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {instructions_path}")
        
        # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        parsed_model_params = None
        if model_params:
            try:
                parsed_model_params = json.loads(model_params)
                logger.info(f"üîß –ü–æ–ª—É—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {parsed_model_params}")
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {e}")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        results = await ollama_service.process_text(
            prompt=prompt,
            input_data=data,
            instructions_file=str(instructions_path) if instructions_path else None,
            task_id=task_id,
            model_name=model_name,
            use_openai=use_openai,
            system_prompt=system_prompt,
            model_params=parsed_model_params
        )
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è task_id: {task_id}")
        
        return JSONResponse(results)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.post("/install-model")
async def install_model(model_name: str = Form(...)):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ Ollama
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏
        
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —É—Å—Ç–∞–Ω–æ–≤–∫–∏
    """
    
    logger.info(f"üì• –ó–∞–ø—Ä–æ—Å –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏: {model_name}")
    
    try:
        if not ollama_service:
            raise HTTPException(status_code=503, detail="–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        success = await ollama_service.install_model(model_name)
        
        if success:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            return JSONResponse({
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
                "model_name": model_name
            })
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")


async def _load_input_data(file_path: Path) -> Any:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
    file_extension = file_path.suffix.lower()
    
    if file_extension == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    elif file_extension in ['.txt', '.md']:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —á–∏—Ç–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å Ollama
        ollama_status = "unknown"
        gpu_status = "unknown"
        device = "cpu"
        
        if torch.cuda.is_available():
            device = "cuda"
            gpu_status = "available"
        else:
            gpu_status = "not_available"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        try:
            import requests
            ollama_host = os.environ.get("OLLAMA_HOST", "localhost:11434")
            response = requests.get(f"http://{ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                ollama_status = "running"
            else:
                ollama_status = "error"
        except:
            ollama_status = "not_available"
        
        return {
            "status": "healthy",
            "service": "ollama_processing",
            "device": device,
            "gpu_status": gpu_status,
            "ollama_status": ollama_status,
            "ollama_host": os.environ.get("OLLAMA_HOST", "localhost:11434"),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "service": "ollama_processing",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@app.get("/model-info")
async def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
    if ollama_service:
        return ollama_service.get_model_info()
    else:
        raise HTTPException(status_code=503, detail="–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")


@app.get("/models")
async def list_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        models = await ollama_service.list_models()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        models_info = []
        for model_name in models:
            models_info.append({
                "name": model_name,
                "size": "Unknown",  # Ollama API –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –Ω–∞–ø—Ä—è–º—É—é
                "modified_at": datetime.now().isoformat()
            })
        
        return {
            "available_models": models_info,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "gpu_available": torch.cuda.is_available()
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        return {"error": str(e)}

@app.post("/models/pull")
async def pull_model(request: ModelPullRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        logger.info(f"üì• –ó–∞–ø—Ä–æ—Å –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏: {request.model_name}")
        
        success = await ollama_service.install_model(request.model_name)
        if success:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {request.model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
                "model": request.model_name
            }
        else:
            return {
                "status": "error",
                "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å {request.model_name}",
                "model": request.model_name
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return {"error": str(e)}



@app.delete("/models/{model_name}")
async def remove_model(model_name: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        logger.info(f"üóëÔ∏è –ó–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        import subprocess
        import asyncio
        
        process = await asyncio.create_subprocess_exec(
            "ollama", "rm", model_name,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        if process.returncode == 0:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞",
                "model": model_name
            }
        else:
            return {
                "status": "error",
                "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}: {stderr.decode()}",
                "model": model_name
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return {"error": str(e)}

@app.get("/models/info")
async def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö –∏ —Å–∏—Å—Ç–µ–º–µ"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        info = ollama_service.get_model_info()
        models = await ollama_service.list_models()
        
        return {
            **info,
            "available_models": models,
            "models_count": len(models)
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö: {e}")
        return {"error": str(e)}


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    try:
        return FileResponse("app/features/ollama_processing/web/index.html")
    except:
        return {"message": "Ollama Processing Service", "docs": "/docs"}

@app.get("/supported-formats")
async def get_supported_formats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    formats = [
        {"extension": ".json", "description": "JSON –¥–∞–Ω–Ω—ã–µ"},
        {"extension": ".txt", "description": "–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"},
        {"extension": ".md", "description": "Markdown —Ñ–∞–π–ª"},
        {"extension": ".csv", "description": "CSV —Ñ–∞–π–ª"},
        {"extension": ".xml", "description": "XML —Ñ–∞–π–ª"}
    ]
    return {"supported_formats": formats}


@app.get("/ollama-status")
async def get_ollama_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama"""
    try:
        import requests
        ollama_host = os.environ.get("OLLAMA_HOST", "localhost:11434")
        response = requests.get(f"http://{ollama_host}/api/tags", timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            models = [model['name'] for model in models_data.get('models', [])]
            return {
                "status": "running",
                "host": ollama_host,
                "available_models": models,
                "model_count": len(models)
            }
        else:
            return {
                "status": "error",
                "host": ollama_host,
                "error": f"HTTP {response.status_code}"
            }
    except Exception as e:
        return {
            "status": "error",
            "host": os.environ.get("OLLAMA_HOST", "localhost:11434"),
            "error": str(e)
        }


if __name__ == "__main__":
    import uvicorn
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    port = int(os.getenv("PORT", 8004))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ {host}:{port}")
    
    uvicorn.run(
        "app.features.ollama_processing.microservice:app",
        host=host,
        port=port,
        reload=False,
        log_level=settings.LOG_LEVEL.lower()
    )
