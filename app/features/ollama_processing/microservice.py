"""
–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query
from fastapi.responses import JSONResponse
import aiofiles

from app.core.config import settings
from app.core.logger import setup_logger
from app.features.ollama_processing.service import OllamaProcessingService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_logger(__name__)

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Ollama Processing Microservice",
    description="–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama",
    version="1.0.0"
)

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
settings.create_directories()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ Ollama
ollama_service = None


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global ollama_service
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ Ollama
        ollama_service = OllamaProcessingService()
        await ollama_service.initialize()
        
        logger.info("‚úÖ –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ Ollama: {e}")
        raise


@app.post("/process")
async def process_text(
    prompt: str = Form(...),
    input_file: Optional[UploadFile] = File(None),
    task_id: str = Form(...),
    instructions_file: Optional[UploadFile] = File(None),
    model_name: str = Form("llama2"),
    use_openai: bool = Form(False),
    system_prompt: Optional[str] = Form(None),
    model_params: Optional[str] = Form(None)
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama –∏–ª–∏ OpenAI
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
    """
    try:
        logger.info(f"üîÑ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É - task_id: {task_id}")
        logger.info(f"ü§ñ –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not use_openai:
            await ensure_model_available(model_name)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        input_data = None
        if input_file:
            content = await input_file.read()
            try:
                input_data = json.loads(content.decode('utf-8'))
            except json.JSONDecodeError:
                input_data = content.decode('utf-8')
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        instructions_content = None
        if instructions_file:
            instructions_path = f"/tmp/instructions_{task_id}.md"
            with open(instructions_path, 'wb') as f:
                f.write(await instructions_file.read())
        else:
            instructions_path = None
        
        # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        parsed_model_params = None
        if model_params:
            try:
                parsed_model_params = json.loads(model_params)
                logger.info(f"üîß –ü–æ–ª—É—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {parsed_model_params}")
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {e}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        result = await ollama_service.process_text(
            prompt=prompt,
            input_data=input_data,
            instructions_file=instructions_path,
            task_id=task_id,
            model_name=model_name,
            use_openai=use_openai,
            system_prompt=system_prompt,
            model_params=parsed_model_params
        )
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        if instructions_path and os.path.exists(instructions_path):
            os.remove(instructions_path)
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        return {
            "error": str(e),
            "task_id": task_id,
            "status": "error"
        }

async def ensure_model_available(model_name: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ—ë –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    try:
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = await ollama_service.list_models()
        logger.info(f"üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
        
        if model_name not in available_models:
            logger.info(f"üì• –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            success = await ollama_service.install_model(model_name)
            if not success:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}")
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        else:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ/–∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise

@app.post("/process-json")
async def process_json_data(
    prompt: str = Form(...),
    data: Optional[Dict[str, Any]] = None,
    task_id: str = Form(...),
    instructions_file: Optional[UploadFile] = File(None),
    model_name: str = Form("llama2"),
    use_openai: bool = Form(False),
    system_prompt: Optional[str] = Form(None),
    model_params: Optional[str] = Form(None)
):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é
    
    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
        data: JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        task_id: ID –∑–∞–¥–∞—á–∏
        instructions_file: –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        use_openai: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI –≤–º–µ—Å—Ç–æ Ollama
        
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    logger.info(f"üì§ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É JSON (task_id: {task_id})")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
        instructions_path = None
        if instructions_file:
            instructions_filename = f"instructions_{task_id}_{instructions_file.filename}"
            instructions_path = settings.UPLOAD_DIR / instructions_filename
            
            async with aiofiles.open(instructions_path, 'wb') as f:
                content = await instructions_file.read()
                await f.write(content)
            
            logger.info(f"üìñ –§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {instructions_path}")
        
        # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        parsed_model_params = None
        if model_params:
            try:
                parsed_model_params = json.loads(model_params)
                logger.info(f"üîß –ü–æ–ª—É—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {parsed_model_params}")
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {e}")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        results = await ollama_service.process_text(
            prompt=prompt,
            input_data=data,
            instructions_file=str(instructions_path) if instructions_path else None,
            task_id=task_id,
            model_name=model_name,
            use_openai=use_openai,
            system_prompt=system_prompt,
            model_params=parsed_model_params
        )
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è task_id: {task_id}")
        
        return JSONResponse(results)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.post("/install-model")
async def install_model(model_name: str = Form(...)):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ Ollama
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏
        
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —É—Å—Ç–∞–Ω–æ–≤–∫–∏
    """
    
    logger.info(f"üì• –ó–∞–ø—Ä–æ—Å –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏: {model_name}")
    
    try:
        if not ollama_service:
            raise HTTPException(status_code=503, detail="–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        success = await ollama_service.install_model(model_name)
        
        if success:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            return JSONResponse({
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
                "model_name": model_name
            })
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")


async def _load_input_data(file_path: Path) -> Any:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
    file_extension = file_path.suffix.lower()
    
    if file_extension == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    elif file_extension in ['.txt', '.md']:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —á–∏—Ç–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "service": "ollama_processing",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/model-info")
async def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
    if ollama_service:
        return ollama_service.get_model_info()
    else:
        raise HTTPException(status_code=503, detail="–°–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")


@app.get("/models")
async def list_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        models = await ollama_service.list_models()
        return {
            "models": models,
            "count": len(models)
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        return {"error": str(e)}

@app.post("/models/install")
async def install_model(model_name: str = Form(...)):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        logger.info(f"üì• –ó–∞–ø—Ä–æ—Å –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏: {model_name}")
        
        success = await ollama_service.install_model(model_name)
        if success:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
                "model": model_name
            }
        else:
            return {
                "status": "error",
                "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}",
                "model": model_name
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return {"error": str(e)}

@app.delete("/models/{model_name}")
async def remove_model(model_name: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        logger.info(f"üóëÔ∏è –ó–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        import subprocess
        import asyncio
        
        process = await asyncio.create_subprocess_exec(
            "ollama", "rm", model_name,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        if process.returncode == 0:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞",
                "model": model_name
            }
        else:
            return {
                "status": "error",
                "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}: {stderr.decode()}",
                "model": model_name
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return {"error": str(e)}

@app.get("/models/info")
async def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö –∏ —Å–∏—Å—Ç–µ–º–µ"""
    try:
        if not ollama_service.is_initialized:
            await ollama_service.initialize()
        
        info = ollama_service.get_model_info()
        models = await ollama_service.list_models()
        
        return {
            **info,
            "available_models": models,
            "models_count": len(models)
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö: {e}")
        return {"error": str(e)}


@app.get("/supported-formats")
async def get_supported_formats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    formats = [
        {"extension": ".json", "description": "JSON –¥–∞–Ω–Ω—ã–µ"},
        {"extension": ".txt", "description": "–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"},
        {"extension": ".md", "description": "Markdown —Ñ–∞–π–ª"},
        {"extension": ".csv", "description": "CSV —Ñ–∞–π–ª"},
        {"extension": ".xml", "description": "XML —Ñ–∞–π–ª"}
    ]
    return {"supported_formats": formats}


@app.get("/ollama-status")
async def get_ollama_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama"""
    try:
        import requests
        ollama_host = os.environ.get("OLLAMA_HOST", "localhost:11434")
        response = requests.get(f"http://{ollama_host}/api/tags", timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            models = [model['name'] for model in models_data.get('models', [])]
            return {
                "status": "running",
                "host": ollama_host,
                "available_models": models,
                "model_count": len(models)
            }
        else:
            return {
                "status": "error",
                "host": ollama_host,
                "error": f"HTTP {response.status_code}"
            }
    except Exception as e:
        return {
            "status": "error",
            "host": os.environ.get("OLLAMA_HOST", "localhost:11434"),
            "error": str(e)
        }


if __name__ == "__main__":
    import uvicorn
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    port = int(os.getenv("PORT", 8004))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ Ollama –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ {host}:{port}")
    
    uvicorn.run(
        "app.features.ollama_processing.microservice:app",
        host=host,
        port=port,
        reload=False,
        log_level=settings.LOG_LEVEL.lower()
    )
