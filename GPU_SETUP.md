# üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- NVIDIA GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA
- NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã –≤–µ—Ä—Å–∏–∏ 450+ 
- Docker Desktop —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU
- NVIDIA Container Toolkit

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ GPU
- **–ú–∏–Ω–∏–º—É–º**: GTX 1060 6GB / RTX 2060 6GB
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: RTX 3070 8GB / RTX 3080 10GB / RTX 4080 16GB
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ**: RTX 4090 24GB / A100 40GB

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ NVIDIA Container Toolkit

### Windows
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ [NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã](https://www.nvidia.com/Download/index.aspx)
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ [Docker Desktop](https://www.docker.com/products/docker-desktop/)
3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)

### Linux
```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

## üß™ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

### 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–æ–≤
```bash
nvidia-smi
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker GPU
```bash
docker run --rm --gpus all nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 nvidia-smi
```

### 3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
```powershell
.\scripts\check-gpu.ps1
```

## üöÄ –ó–∞–ø—É—Å–∫ —Å GPU

### 1. –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–∞ —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
```powershell
.\scripts\docker-build-transcription-gpu.ps1
```

### 2. –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Å GPU
```powershell
.\scripts\docker-run-transcription-gpu.ps1
```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã GPU
```powershell
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
docker logs transcription-service

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
docker exec transcription-service nvidia-smi
```

## üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ CPU vs GPU

| –ú–æ–¥–µ–ª—å | CPU (Intel i7) | GPU (RTX 3080) | –£—Å–∫–æ—Ä–µ–Ω–∏–µ |
|--------|----------------|----------------|-----------|
| tiny   | ~30 —Å–µ–∫        | ~5 —Å–µ–∫         | 6x        |
| base   | ~60 —Å–µ–∫        | ~10 —Å–µ–∫        | 6x        |
| small  | ~120 —Å–µ–∫       | ~20 —Å–µ–∫        | 6x        |
| medium | ~300 —Å–µ–∫       | ~50 —Å–µ–∫        | 6x        |
| large  | ~600 —Å–µ–∫       | ~100 —Å–µ–∫       | 6x        |

*–í—Ä–µ–º—è —É–∫–∞–∑–∞–Ω–æ –¥–ª—è –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –¥–ª–∏–Ω–æ–π 1 –º–∏–Ω—É—Ç–∞*

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏

### –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
–í —Ñ–∞–π–ª–µ `app/features/transcription/service.py`:
```python
self.model_size = "base"  # tiny, base, small, medium, large-v2, large-v3
```

### –¢–∏–ø –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- **GPU**: `float16` (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
- **CPU**: `int8` (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

### Batch size
- **GPU**: 16 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
- **CPU**: 4 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –û—à–∏–±–∫–∞: "CUDA out of memory"
```bash
# –£–º–µ–Ω—å—à–∏—Ç–µ batch size –≤ –∫–æ–¥–µ
batch_size=8 if self.device == "cuda" else 4

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
self.model_size = "small"
```

### –û—à–∏–±–∫–∞: "NVIDIA Container Toolkit not found"
```bash
# –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA Container Toolkit
# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Docker Desktop
```

### –û—à–∏–±–∫–∞: "No CUDA devices available"
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Docker GPU –ø–æ–¥–¥–µ—Ä–∂–∫—É
docker run --rm --gpus all nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 nvidia-smi
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
```bash
# –í —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
watch -n 1 nvidia-smi

# –í –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
docker exec transcription-service nvidia-smi
```

### –õ–æ–≥–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
```bash
docker logs -f transcription-service
```

## üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU —Å 8GB+ –ø–∞–º—è—Ç–∏
2. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å `medium` –∏–ª–∏ `large`
3. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SSD –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤

### –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤:
1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å `tiny` –∏–ª–∏ `base`
2. –û–≥—Ä–∞–Ω–∏—á—å—Ç–µ batch size
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [WhisperX Documentation](https://github.com/m-bain/whisperX)
- [PyTorch CUDA](https://pytorch.org/docs/stable/cuda.html)

