# –°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
Write-Host "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU..." -ForegroundColor Green

# –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA –¥—Ä–∞–π–≤–µ—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ
Write-Host "üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA –¥—Ä–∞–π–≤–µ—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ:" -ForegroundColor Cyan
try {
    $nvidiaSmi = nvidia-smi 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Host "‚úÖ NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:" -ForegroundColor Green
        Write-Host $nvidiaSmi -ForegroundColor Gray
    } else {
        Write-Host "‚ùå NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã" -ForegroundColor Red
    }
} catch {
    Write-Host "‚ùå –ö–æ–º–∞–Ω–¥–∞ nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞" -ForegroundColor Red
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
Write-Host "`nüê≥ –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:" -ForegroundColor Cyan
try {
    $dockerGpu = docker run --rm --gpus all nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 nvidia-smi 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Host "‚úÖ Docker –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GPU:" -ForegroundColor Green
        Write-Host $dockerGpu -ForegroundColor Gray
    } else {
        Write-Host "‚ùå Docker –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GPU" -ForegroundColor Red
        Write-Host "üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA Container Toolkit:" -ForegroundColor Yellow
        Write-Host "   https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" -ForegroundColor Yellow
    }
} catch {
    Write-Host "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Docker GPU" -ForegroundColor Red
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch —Å CUDA
Write-Host "`nüî• –ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch —Å CUDA:" -ForegroundColor Cyan
try {
    $pytorchCheck = docker run --rm --gpus all transcription-service:latest python3 -c "
import torch
print(f'PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}')
print(f'CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}')
    print(f'GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} –ì–ë')
else:
    print('CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞')
" 2>$null

    if ($LASTEXITCODE -eq 0) {
        Write-Host "‚úÖ PyTorch —Å CUDA —Ä–∞–±–æ—Ç–∞–µ—Ç:" -ForegroundColor Green
        Write-Host $pytorchCheck -ForegroundColor Gray
    } else {
        Write-Host "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ PyTorch" -ForegroundColor Red
    }
} catch {
    Write-Host "‚ùå –û–±—Ä–∞–∑ transcription-service:latest –Ω–µ –Ω–∞–π–¥–µ–Ω" -ForegroundColor Red
    Write-Host "üí° –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ –æ–±—Ä–∞–∑: .\scripts\docker-build-transcription-gpu.ps1" -ForegroundColor Yellow
}

Write-Host "`nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!" -ForegroundColor Green
